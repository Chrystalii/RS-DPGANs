{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ER5Q8cbTeWit"
   },
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjGtjZWokG36"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "n0L-DdHOeWjM"
   },
   "outputs": [],
   "source": [
    "# Method obtained from https://stackoverflow.com/questions/41123879/numpy-random-choice-in-tensorflow\n",
    "def _random_choice(inputs, n_samples):\n",
    "    \"\"\"\n",
    "    With replacement.\n",
    "    Params:\n",
    "      inputs (Tensor): Shape [n_states, n_features]\n",
    "      n_samples (int): The number of random samples to take.\n",
    "    Returns:\n",
    "      sampled_inputs (Tensor): Shape [n_samples, n_features]\n",
    "    \"\"\"\n",
    "    # (1, n_states) since multinomial requires 2D logits.\n",
    "    uniform_log_prob = tf.expand_dims(tf.zeros(tf.shape(inputs)[0]), 0)\n",
    "\n",
    "    ind = tf.compat.v1.multinomial(uniform_log_prob, n_samples)\n",
    "    ind = tf.squeeze(ind, 0, name=\"random_choice_ind\")  # (n_samples,)\n",
    "\n",
    "    return tf.gather(inputs, ind, name=\"random_choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_2WUvm2LeWjS",
    "outputId": "a7314624-e3b6-4a0a-ae97-900cd543b8f4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "ConvergenceWarning('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AqszHUqweWjW"
   },
   "outputs": [],
   "source": [
    "# Method obtained from https://github.com/reihaneh-torkzadehmahani/DP-CGAN\n",
    "def compute_fpr_tpr_roc(Y_test, Y_score):\n",
    "    n_classes = Y_score.shape[1]\n",
    "    false_positive_rate = dict()\n",
    "    true_positive_rate = dict()\n",
    "    roc_auc = dict()\n",
    "    for class_cntr in range(n_classes):\n",
    "        false_positive_rate[class_cntr], true_positive_rate[class_cntr], _ = roc_curve(Y_test[:, class_cntr],\n",
    "                                                                                       Y_score[:, class_cntr])\n",
    "        roc_auc[class_cntr] = auc(false_positive_rate[class_cntr], true_positive_rate[class_cntr])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    false_positive_rate[\"micro\"], true_positive_rate[\"micro\"], _ = roc_curve(Y_test.ravel(), Y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(false_positive_rate[\"micro\"], true_positive_rate[\"micro\"])\n",
    "\n",
    "    return false_positive_rate, true_positive_rate, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuCo5voGT-Jb"
   },
   "source": [
    "## Modified Optimizer for DP\n",
    "\n",
    "The optimizer below is a modification of the original from TF Privacy, [available here](https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/optimizers/dp_optimizer.py) to allow setting different values of noise multipliers and clipping factor on different steps of the optimization.\n",
    "\n",
    "The main modification lies on the `compute_gradients` method, which now includes:\n",
    "- *curr_noise_mult*: Current noise_multiplier\n",
    "- *curr_norm_clip*: Current L2 norm clipping factor\n",
    "\n",
    "On every step of the optimization we now additionally pass these parameters to control the privacy effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hOBQELw7mo9Z"
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import collections\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
    "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
    "\n",
    "def make_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
    "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
    "  child_code = cls.compute_gradients.__code__\n",
    "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
    "  if child_code is not parent_code:\n",
    "    logging.warning(\n",
    "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
    "        'method compute_gradients(). Check to ensure that '\n",
    "        'make_optimizer_class() does not interfere with overridden version.',\n",
    "        cls.__name__)\n",
    "\n",
    "  class DPOptimizerClass(cls):\n",
    "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
    "\n",
    "    _GlobalState = collections.namedtuple(\n",
    "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dp_sum_query,\n",
    "        num_microbatches=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
    "        **kwargs):\n",
    "      \"\"\"Initialize the DPOptimizerClass.\n",
    "\n",
    "      Args:\n",
    "        dp_sum_query: DPQuery object, specifying differential privacy\n",
    "          mechanism to use.\n",
    "        num_microbatches: How many microbatches into which the minibatch is\n",
    "          split. If None, will default to the size of the minibatch, and\n",
    "          per-example gradients will be computed.\n",
    "        unroll_microbatches: If true, processes microbatches within a Python\n",
    "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
    "          raises an exception.\n",
    "      \"\"\"\n",
    "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
    "      self._dp_sum_query = dp_sum_query\n",
    "      self._num_microbatches = num_microbatches\n",
    "      self._global_state = self._dp_sum_query.initial_global_state()\n",
    "      # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
    "      # Beware: When num_microbatches is large (>100), enabling this parameter\n",
    "      # may cause an OOM error.\n",
    "      self._unroll_microbatches = unroll_microbatches\n",
    "\n",
    "    def compute_gradients(self,\n",
    "                          loss,\n",
    "                          var_list,\n",
    "                          gate_gradients=GATE_OP,\n",
    "                          aggregation_method=None,\n",
    "                          colocate_gradients_with_ops=False,\n",
    "                          grad_loss=None,\n",
    "                          gradient_tape=None,\n",
    "                          curr_noise_mult=0,\n",
    "                          curr_norm_clip=1):\n",
    "\n",
    "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
    "                                                           curr_norm_clip*curr_noise_mult)\n",
    "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
    "                                                                curr_norm_clip*curr_noise_mult)\n",
    "      \n",
    "\n",
    "      # TF is running in Eager mode, check we received a vanilla tape.\n",
    "      if not gradient_tape:\n",
    "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
    "\n",
    "      vector_loss = loss()\n",
    "      if self._num_microbatches is None:\n",
    "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
    "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
    "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
    "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
    "\n",
    "      def process_microbatch(i, sample_state):\n",
    "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
    "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
    "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
    "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
    "        return sample_state\n",
    "    \n",
    "      for idx in range(self._num_microbatches):\n",
    "        sample_state = process_microbatch(idx, sample_state)\n",
    "\n",
    "      if curr_noise_mult > 0:\n",
    "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
    "      else:\n",
    "        grad_sums = sample_state\n",
    "\n",
    "      def normalize(v):\n",
    "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
    "\n",
    "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
    "      grads_and_vars = final_grads#list(zip(final_grads, var_list))\n",
    "    \n",
    "      return grads_and_vars\n",
    "\n",
    "  return DPOptimizerClass\n",
    "\n",
    "\n",
    "def make_gaussian_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
    "\n",
    "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
    "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2_norm_clip,\n",
    "        noise_multiplier,\n",
    "        num_microbatches=None,\n",
    "        ledger=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
    "        **kwargs):\n",
    "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
    "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
    "\n",
    "      if ledger:\n",
    "        dp_sum_query = privacy_ledger.QueryWithLedger(dp_sum_query,\n",
    "                                                      ledger=ledger)\n",
    "\n",
    "      super(DPGaussianOptimizerClass, self).__init__(\n",
    "          dp_sum_query,\n",
    "          num_microbatches,\n",
    "          unroll_microbatches,\n",
    "          *args,\n",
    "          **kwargs)\n",
    "\n",
    "    @property\n",
    "    def ledger(self):\n",
    "      return self._dp_sum_query.ledger\n",
    "\n",
    "  return DPGaussianOptimizerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iwrPjV66eWjf"
   },
   "outputs": [],
   "source": [
    "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
    "DPGradientDescentGaussianOptimizer_NEW = make_gaussian_optimizer_class(GradientDescentOptimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Sb7NTLhqyh8"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "D4yIv7MAqn9a",
    "outputId": "c708f4d9-e2e0-4e5d-cbfe-9a002759a0b7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as io\n",
    "\n",
    "dataset = io.loadmat('indianpines_dataset.mat')\n",
    "number_of_bands = int(dataset['number_of_bands'])\n",
    "number_of_rows = int(dataset['number_of_rows'])\n",
    "number_of_columns = int(dataset['number_of_columns'])\n",
    "pixels = np.transpose(dataset['pixels'])\n",
    "\n",
    "groundtruth = io.loadmat('indianpines_gt.mat')\n",
    "gt = np.transpose(groundtruth['pixels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the dataset (standard procedure)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "pixels = sc.fit_transform(pixels)\n",
    "\n",
    "# colors for each category in the dataset\n",
    "indianpines_colors = np.array([[255, 255, 255],\n",
    "                               [255, 254, 137], [3,  28,  241], [255, 89,    1], [5,   255, 133],\n",
    "                               [255,   2, 251], [89,  1,  255], [3,   171, 255], [12,  255,   7],\n",
    "                               [172, 175,  84], [160, 78, 158], [101, 173, 255], [60,   91, 112],\n",
    "                               [104, 192,  63], [139, 69,  46], [119, 255, 172], [254, 255,   3]])\n",
    "\n",
    "# normalize in the range of 0 and 1 for displaying\n",
    "import sklearn.preprocessing\n",
    "indianpines_colors = sklearn.preprocessing.minmax_scale(indianpines_colors, feature_range=(0, 1))\n",
    "pixels_normalized = sklearn.preprocessing.minmax_scale(pixels, feature_range=(0, 1))\n",
    "\n",
    "# build the RGB Image\n",
    "gt_thematic_map = np.zeros(shape=(number_of_rows, number_of_columns, 3))\n",
    "cont = 0\n",
    "for i in range(number_of_rows):\n",
    "    for j in range(number_of_columns):\n",
    "        gt_thematic_map[i, j, :] = indianpines_colors[gt[cont, 0]]\n",
    "        cont += 1\n",
    "\n",
    "# names of the categories in the dataset\n",
    "indianpines_class_names = ['background',\n",
    "                           'alfalfa',           'corn-notill',               'corn-min',               'corn',\n",
    "                           'grass/pasture',     'grass/trees',    'grass/pasture-mowed',      'hay-windrowed',\n",
    "                           'oats',          'soybeans-notill',           'soybeans-min',      'soybean-clean',\n",
    "                           'wheat',                   'woods', 'bldg-grass-tree-drives', 'stone-steel towers']\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(gt_thematic_map)\n",
    "COND_num_classes = 17\n",
    "y = gt\n",
    "x = gt_thematic_map.reshape(gt_thematic_map.shape[0]*gt_thematic_map.shape[1], 3)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.75, random_state=0)\n",
    "train_dataset = np.expand_dims(x_train, axis = 1)\n",
    "test_dataset = np.expand_dims(x_test, axis = 1)\n",
    "print (train_dataset.shape)\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "train_labels_vec = np.zeros((len(train_labels), COND_num_classes), dtype='float32')\n",
    "test_labels_vec = np.zeros((len(test_labels), COND_num_classes), dtype='float32')\n",
    "for i, label in enumerate(train_labels):\n",
    "    train_labels_vec[i, int(train_labels[i])] = 1.0\n",
    "    \n",
    "for i, label in enumerate(test_labels):\n",
    "    test_labels_vec[i, int(test_labels[i])] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehOkmDl__ZyH"
   },
   "source": [
    "## C-GAN Models\n",
    "\n",
    "Both Generator and Discriminator follow simple architectures, with fully connected neural networks.\n",
    "\n",
    "We emphasize the use of C-GAN, therefore conditioning the models to the label information - notice the additional input on both networks below for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "byqg8h3FeWkA"
   },
   "outputs": [],
   "source": [
    "# Dimension of Latent Space - Does NOT affect DP-EPSILON\n",
    "Z_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PVBOYFJskMTZ"
   },
   "outputs": [],
   "source": [
    "def make_generator_model_FCC():\n",
    "    # INPUT: label input\n",
    "    in_label = layers.Input(shape=(COND_num_classes,))\n",
    "\n",
    "    # INPUT: image generator input\n",
    "    in_lat = layers.Input(shape=(Z_DIM,))\n",
    "\n",
    "    # MERGE\n",
    "    merge = layers.concatenate([in_lat, in_label], axis=1)\n",
    "\n",
    "    ge1 = layers.Dense(128, use_bias=True)(merge)\n",
    "    ge1 = layers.ReLU()(ge1)\n",
    "\n",
    "    ge2 = layers.Dense(3, use_bias=True, activation=\"tanh\")(ge1)\n",
    "    ge2 = layers.ReLU()(ge2)\n",
    "    out_layer = layers.Reshape((1,3))(ge2)\n",
    "\n",
    "    model = models.Model([in_lat, in_label], out_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model_FCC():\n",
    "    # INPUT: Label\n",
    "    in_label = layers.Input(shape=(COND_num_classes,))\n",
    "\n",
    "    # INPUT: Image\n",
    "    in_image = layers.Input(shape=(1,3))\n",
    "    in_image_b = layers.Flatten()(in_image)\n",
    "\n",
    "    # MERGE\n",
    "    merge = layers.concatenate([in_image_b, in_label], axis=1)\n",
    "\n",
    "    ge1 = layers.Dense(128, use_bias=True)(merge)\n",
    "    ge1 = layers.ReLU()(ge1)\n",
    "\n",
    "    ge2 = layers.Dense(3, use_bias=True)(ge1)\n",
    "    ge2 = layers.ReLU()(ge2)\n",
    "    out_layer = layers.Reshape((1,3))(ge2)\n",
    "\n",
    "    model = models.Model([in_image, in_label], out_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaloAxCSeWkH"
   },
   "source": [
    "### Initiate and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "cOuBI2X2eWkI",
    "outputId": "d17a420b-8890-424d-a504-b705f95b26ad"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model_FCC()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "mmntW8vFeWkM",
    "outputId": "57d0569b-514b-4f14-f581-d12e9c7d1e45"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model_FCC()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "A_b5QPsCwEIW",
    "outputId": "3df4fd67-300e-43d3-bc3b-bee5e103f79f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test GEN created\n",
    "noise = tf.Variable(tf.random.normal([1, Z_DIM]))\n",
    "noise_label = tf.Variable(np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,0,0,0,0,0,0], dtype='float32').reshape((1,17)))\n",
    "print(noise.shape)\n",
    "print(noise_label.shape)\n",
    "generated_image = generator([noise, noise_label], training=False).numpy()\n",
    "plt.imshow(generated_image)\n",
    "\n",
    "# Test DISC created\n",
    "decision = discriminator([generated_image, noise_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtGqeGSUeWkS"
   },
   "source": [
    "### Loss and Updates\n",
    "\n",
    "- Please note that, during the training step of the Discriminator `train_step_DISC`, we **combine gradients** from both real and generated on a single update step into `sanitized_grads_and_vars`, following the approach from [Torkzadehmahani et al. 2019](http://openaccess.thecvf.com/content_CVPRW_2019/papers/CV-COPS/Torkzadehmahani_DP-CGAN_Differentially_Private_Synthetic_Data_and_Label_Generation_CVPRW_2019_paper.pdf).\n",
    "- When learning from the **real/training dataset** we <u>clip and add noise</u> to the gradients of the Discriminator.\n",
    "- When learning from the **generated data** we <u>only clip</u> the gradients of the Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Yj0E2qo3kHb0"
   },
   "outputs": [],
   "source": [
    "cross_entropy_DISC = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "cross_entropy_GEN = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Notice the use of `tf.function`: This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step_DISC(images, labels, noise, labels_to_gen):    \n",
    "    with tf.GradientTape(persistent=True) as disc_tape_real:\n",
    "        # This dummy call is needed to obtain the var list.\n",
    "        dummy = discriminator([images, labels], training=True)\n",
    "        var_list = discriminator.trainable_variables\n",
    "        \n",
    "        # In Eager mode, the optimizer takes a function that returns the loss.\n",
    "        def loss_fn_real():\n",
    "            real_output = discriminator([images, labels], training=True)\n",
    "            disc_real_loss = cross_entropy_DISC(tf.ones_like(real_output), real_output)\n",
    "            return disc_real_loss\n",
    "        \n",
    "        grads_and_vars_real = discriminator_optimizer.compute_gradients(loss_fn_real, \n",
    "                                                                        var_list, \n",
    "                                                                        gradient_tape=disc_tape_real, \n",
    "                                                                        curr_noise_mult=NOISE_MULT,\n",
    "                                                                        curr_norm_clip=NORM_CLIP)\n",
    "        \n",
    "        # In Eager mode, the optimizer takes a function that returns the loss.\n",
    "        def loss_fn_fake():\n",
    "            generated_images = generator([noise, labels_to_gen], training=True)\n",
    "            fake_output = discriminator([generated_images, labels_to_gen], training=True)\n",
    "            disc_fake_loss = cross_entropy_DISC(tf.zeros_like(fake_output), fake_output)\n",
    "            return disc_fake_loss\n",
    "        \n",
    "        grads_and_vars_fake = discriminator_optimizer.compute_gradients(loss_fn_fake,\n",
    "                                                                        var_list, \n",
    "                                                                        gradient_tape=disc_tape_real,\n",
    "                                                                        curr_noise_mult=0,\n",
    "                                                                        curr_norm_clip=NORM_CLIP)\n",
    "        disc_loss_r = loss_fn_real()\n",
    "        disc_loss_f = loss_fn_fake()\n",
    "        \n",
    "        s_grads_and_vars = [(grads_and_vars_real[idx] + grads_and_vars_fake[idx])\n",
    "                            for idx in range(len(grads_and_vars_real))]\n",
    "        sanitized_grads_and_vars = list(zip(s_grads_and_vars, var_list))\n",
    "        \n",
    "        discriminator_optimizer.apply_gradients(sanitized_grads_and_vars)\n",
    "        \n",
    "    return(disc_loss_r, disc_loss_f)\n",
    "\n",
    "# Notice the use of `tf.function`: This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step_GEN(labels, noise):\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_images = generator([noise, labels], training=True)\n",
    "        fake_output = discriminator([generated_images, labels], training=True)\n",
    "        # if the generator is performing well, the discriminator will classify the fake images as real (or 1)\n",
    "        gen_loss = cross_entropy_GEN(tf.ones_like(fake_output), fake_output)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "    return(gen_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_dir = 'results'\n",
    "checkpoint_dir = result_dir + '/training_checkpoints'\n",
    "\n",
    "def checkpoint_name(title):  \n",
    "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt__\" + str(title))\n",
    "  return(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_dir = result_dir +'/images'\n",
    "\n",
    "def generate_and_save_images(title, model, epoch, test_input, test_label):\n",
    "  # Notice `training` is set to False: This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model([test_input, test_label], training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(2,17))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(17, 1, i+1)\n",
    "      prediction = predictions.numpy()\n",
    "        \n",
    "      plt.imshow(prediction)\n",
    "      plt.axis('off')\n",
    "\n",
    "  #plt.savefig(images_dir + '/' + title + '___image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p5AYc6qqeWkW"
   },
   "source": [
    "### Train function definition\n",
    "\n",
    "- The Generator receives labels as input, in addition to noise, but since the labels are considered sensitive, as part of the training data, the Generator will **NOT** see/receive them.\n",
    "- In this sense, we get **uniform random samples** of the possible labels to pass to the Generator.\n",
    "- Therefore, we do **NOT** use DP-SGD on the Generator, since only the Discriminator trains using the sensitive training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KBYM_t3heWkX"
   },
   "outputs": [],
   "source": [
    "def train(dataset, title, verbose):\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        i_gen = 0\n",
    "        for image_batch, label_batch in dataset:\n",
    "            if verbose:\n",
    "                print(\"Iteration: \" + str(i_gen+1))\n",
    "            \n",
    "            noise = tf.random.normal([BATCH_SIZE, Z_DIM])\n",
    "            labels_to_gen = _random_choice(labels_gen_vec, BATCH_SIZE)\n",
    "    \n",
    "            d_loss_r, d_loss_f = train_step_DISC(image_batch, label_batch, noise, labels_to_gen)\n",
    "            if verbose:\n",
    "                print(\"Loss DISC Real: \" + str(tf.reduce_mean(d_loss_r)))\n",
    "                print(\"Loss DISC Fake: \" + str(tf.reduce_mean(d_loss_f)))\n",
    "\n",
    "            if (i_gen + 1) % N_DISC == 0:\n",
    "                g_loss_f = train_step_GEN(labels_to_gen, noise)\n",
    "                if verbose:\n",
    "                    print(\"Loss GEN Fake:: \" + str(g_loss_f))\n",
    "\n",
    "            i_gen = i_gen + 1\n",
    "\n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(title,\n",
    "                                 generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed,\n",
    "                                 seed_labels)\n",
    "        \n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "        # Save the model\n",
    "        #checkpoint.save(file_prefix = checkpoint_name(title + \"__epoch=\" + str(epoch) + \"__\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqMlVrs5G9bI"
   },
   "source": [
    "---\n",
    "\n",
    "## Parameters\n",
    "\n",
    "Specific parameters due to DP-SGD:\n",
    "- **NR_MICROBATCHES** (microbatches - int): Each batch of data (of size BATCH_SIZE) is split into smaller units called microbatches. So naturally NR_MICROBATCHES should evenly divide BATCH_SIZE. If NR_MICROBATCHES = BATCH_SIZE then every training example is a microbatch, clipped individually and with noise added to the average. As NR_MICROBATCHES decreases, we have more examples in a single microbatch, where *averaged* microbatches are clipped and noise is added to the *average* of averaged microbatches.\n",
    "- **NORM_CLIP** (l2_norm_clip - float) - The maximum Euclidean (L2) norm of each individual (or microbatch) gradient. To enforce such maximum norm gradients are clipped, which bounds the optimizer's sensitivity to individual training data.\n",
    "- **NOISE_MULT** (noise_multiplier - float) - The amount of noise sampled and added to gradients during training. Generally, more noise gives better privacy, which often, but not necessarily, lowers utility.\n",
    "    - Please have in mind that the actual noise added in practice is sampled from a Gaussian distribution with mean zero and standard deviation NORM_CLIP * NOISE_MULT.\n",
    "    - Therefore, a larger NORM_CLIP may pass more signal from the data via gradients, but it also increases the noise added to the gradients.\n",
    "    - TF Privacy's authors [have already pointed out](http://www.cleverhans.io/privacy/2019/03/26/machine-learning-with-differential-privacy-in-tensorflow.html) that setting NR_MICROBATCHES trades off performance (e.g. NR_MICROBATCHES = 1) with utility (e.g. NR_MICROBATCHES = BATCH_SIZE).\n",
    "- **DP_DELTA**: Delta from the DP definition. We emphasize that DP_DELTA needs to be smaller than 1/BUFFER_SIZE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zAn0CETqHzAz"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_dataset) # Total size of training data\n",
    "BATCH_SIZE = 256\n",
    "NR_MICROBATCHES = 64 # Each batch of data is split in smaller units called microbatches.\n",
    "\n",
    "\n",
    "NORM_CLIP = 1.1 # Does NOT affect EPSILON, but increases NOISE on gradients\n",
    "NOISE_MULT = 1.15\n",
    "\n",
    "\n",
    "DP_DELTA = 1e-4 # Needs to be smaller than 1/BUFFER_SIZE\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "N_DISC = 1 # Number of times we train DISC before training GEN once\n",
    "\n",
    "\n",
    "# Learning Rate for DISCRIMINATOR\n",
    "LR_DISC = tf.compat.v1.train.polynomial_decay(learning_rate=0.150,\n",
    "                                              global_step=tf.compat.v1.train.get_or_create_global_step(),\n",
    "                                              decay_steps=10000,\n",
    "                                              end_learning_rate=0.052,\n",
    "                                              power=1)\n",
    "\n",
    "if BATCH_SIZE % NR_MICROBATCHES != 0:\n",
    "    raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBrUj8tneWkf"
   },
   "source": [
    "### Get DP epsilon from parameters\n",
    "\n",
    "- Instead of updating and consulting the moments accountant on each step of training, we just previously check the epsilon we obtain from the given parameters.\n",
    "- Therefore, we can just quickly keep manually adjusting the parameters above to reach our desired epsilon below, and avoid extra computation during training.\n",
    "- Moreover, this allows a better understanding of the privacy implications of each parameter above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "VocXhKB5lSEV",
    "outputId": "b8a3249b-f5fc-4d94-f1e4-8c2663132ee7"
   },
   "outputs": [],
   "source": [
    "# Obtain DP_EPSILON\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n = BUFFER_SIZE, \n",
    "                                              batch_size = BATCH_SIZE, \n",
    "                                              noise_multiplier = NOISE_MULT, \n",
    "                                              epochs = EPOCHS, \n",
    "                                              delta = DP_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R1WvIihNKwXI",
    "outputId": "63769e35-083a-474e-9c09-fc71be7d391d"
   },
   "outputs": [],
   "source": [
    "# SD of noise that will be added to gradients: sanity check\n",
    "NOISE_MULT*NORM_CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMMbWnKoNa99"
   },
   "source": [
    "### Optimizers\n",
    "\n",
    "Instantiating optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UovjSm3wKG3h"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "discriminator_optimizer = DPGradientDescentGaussianOptimizer_NEW(\n",
    "   learning_rate = LR_DISC,\n",
    "   l2_norm_clip = NORM_CLIP,\n",
    "   noise_multiplier = NOISE_MULT,\n",
    "   num_microbatches = NR_MICROBATCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6hVijBPlfg8"
   },
   "source": [
    "---\n",
    "\n",
    "## TRAINING\n",
    "\n",
    "- We emphasize here that when batching our training dataset, DP requires random shuffling.\n",
    "- To help track the progress of our GAN, we fix some seeds for labels and noise for the generator, and constantly plot the generated images. Below we create one seed for each of the 10 classes on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fIw5KKbyeWks"
   },
   "outputs": [],
   "source": [
    "# Create/reinitiate models\n",
    "generator = make_generator_model_FCC()\n",
    "discriminator = make_discriminator_model_FCC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tGI0EgGjoTrQ"
   },
   "outputs": [],
   "source": [
    "# Create checkpoint structure\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nTJgxxIqqw5n"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "# Batch and random shuffle training data\n",
    "train_datasets = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_dataset, train_labels_vec)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# Fix some seeds to help visualize progress\n",
    "seed = tf.random.normal([17, Z_DIM])\n",
    "seed_labels = tf.Variable(np.diag(np.full(17,1)).reshape((17,17)), dtype='float32')\n",
    "\n",
    "# To be used for sampling random labels to pass to generator\n",
    "labels_gen_vec = np.zeros((17, COND_num_classes), dtype='float32')\n",
    "for i in [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]:\n",
    "  labels_gen_vec[i, int(i)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BVDi7vlNqBlB"
   },
   "outputs": [],
   "source": [
    "# GIVES CURRENT TRIAL A NAME - Suggestion: from parameters used\n",
    "training_title = 'eps9.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "colab_type": "code",
    "id": "IL6rZttmrBOp",
    "outputId": "cdd783e3-4957-463f-e874-f05d6dc65815"
   },
   "outputs": [],
   "source": [
    "# STARTS TRAINING\n",
    "train(train_datasets, training_title, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqYmjrrneWk4"
   },
   "source": [
    "---\n",
    "\n",
    "## VALIDATION\n",
    "\n",
    "We consider that the GAN training is performed specifically with the goal of publicly sharing the generated data to allow others to train a ML model. \n",
    "\n",
    "For this reason, we validate the results by training models on the generated data, and finally, after deciding on a single final GAN properly validated, we test the results by applying on the real test data the models trained on the generated data.\n",
    "\n",
    "\n",
    "### Choose model to use\n",
    "Select one of the trials (a fixed GAN) to validate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwQ0dU4reWk_"
   },
   "source": [
    "### Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9860snK5eWk_"
   },
   "outputs": [],
   "source": [
    "# Number of images to generate\n",
    "N_GEN = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9DqLFm1Ix7cK"
   },
   "outputs": [],
   "source": [
    "N_GEN_per_CLASS = np.int(N_GEN/COND_num_classes)\n",
    "\n",
    "tf.random.set_seed(17)\n",
    "COND_GEN = int(COND_num_classes * N_GEN_per_CLASS)\n",
    "noise_GEN = tf.random.normal([COND_GEN, Z_DIM])\n",
    "labels_GEN = tf.Variable(np.array( [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS + \n",
    "                                   [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]*N_GEN_per_CLASS +\n",
    "                                   [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]*N_GEN_per_CLASS,\n",
    "                                   dtype='float32').reshape((COND_GEN,COND_num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jkV2uEM4ykxF"
   },
   "outputs": [],
   "source": [
    "images_GEN = generator([noise_GEN, labels_GEN], training=False)\n",
    "images_flat = layers.Flatten()(images_GEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wqAmbPcSy_aP"
   },
   "outputs": [],
   "source": [
    "labels_flat = tf.Variable(np.array([0]*N_GEN_per_CLASS + \n",
    "                                   [1]*N_GEN_per_CLASS +\n",
    "                                   [2]*N_GEN_per_CLASS +\n",
    "                                   [3]*N_GEN_per_CLASS +\n",
    "                                   [4]*N_GEN_per_CLASS +\n",
    "                                   [5]*N_GEN_per_CLASS +\n",
    "                                   [6]*N_GEN_per_CLASS +\n",
    "                                   [7]*N_GEN_per_CLASS +\n",
    "                                   [8]*N_GEN_per_CLASS +\n",
    "                                   [9]*N_GEN_per_CLASS + \n",
    "                                   [10]*N_GEN_per_CLASS +\n",
    "                                   [11]*N_GEN_per_CLASS +\n",
    "                                   [12]*N_GEN_per_CLASS +\n",
    "                                   [13]*N_GEN_per_CLASS +\n",
    "                                   [14]*N_GEN_per_CLASS +\n",
    "                                   [15]*N_GEN_per_CLASS +\n",
    "                                   [16]*N_GEN_per_CLASS,\n",
    "                                   dtype='float32').reshape((COND_GEN,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2npDQqY3zWHt"
   },
   "outputs": [],
   "source": [
    "Y_train = labels_flat[:images_flat.shape[0]]\n",
    "X_train = images_flat\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "Y_train_org = label_binarize(Y_train, classes=classes)\n",
    "Y_train_vec = layers.Flatten()(Y_train_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qtb06W4a-nWD"
   },
   "source": [
    "### Get validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UKhZFobreWlg"
   },
   "outputs": [],
   "source": [
    "##### Vanilla Neural Network\n",
    "\n",
    "tf.random.set_seed(100)\n",
    "classifier_NN = OneVsRestClassifier(MLPClassifier(random_state=2, alpha=1))\n",
    "NN_model = classifier_NN.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "NjlKXGnHbmpk",
    "outputId": "8734db43-ef0c-43d7-916d-edc20b85f9f5"
   },
   "outputs": [],
   "source": [
    "# ROC per class: Validating on REAL training dataset\n",
    "Y_score = NN_model.predict_proba(np.squeeze(train_dataset, axis = 1))\n",
    "false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(np.array(train_labels_vec), Y_score)\n",
    "[str(au) + \" = \" + str(roc_auc[au]) for au in roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_thematic_map = np.zeros(shape=(number_of_rows, number_of_columns, 3))\n",
    "predicted_dataset = NN_model.predict(x).astype(int)\n",
    "cont = 0\n",
    "for i in range(number_of_rows):\n",
    "    for j in range(number_of_columns):\n",
    "        gt_thematic_map[i, j, :] = indianpines_colors[gt[cont, 0]]\n",
    "        predicted_thematic_map[i, j, :] = indianpines_colors[predicted_dataset[cont]]\n",
    "        cont += 1\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "columns = 2\n",
    "rows = 1\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(gt_thematic_map)\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(predicted_thematic_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6MR3vZhXeWlX"
   },
   "outputs": [],
   "source": [
    "##### Logistic Regression\n",
    "\n",
    "tf.random.set_seed(100)\n",
    "classifier_LR = OneVsRestClassifier(LogisticRegression(solver='lbfgs', \n",
    "                                                       multi_class='multinomial', \n",
    "                                                       random_state=2))\n",
    "LR_model = classifier_LR.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6SRt9hYmXDrc",
    "outputId": "4928f7e8-b207-4fd0-f62d-205e0d8766f8"
   },
   "outputs": [],
   "source": [
    "# ROC per class: Validating on REAL training dataset\n",
    "Y_score = LR_model.predict_proba(np.squeeze(train_dataset, axis = 1))\n",
    "false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(np.array(train_labels_vec), Y_score)\n",
    "[str(au) + \" = \" + str(roc_auc[au]) for au in roc_auc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJwknz5neWlj"
   },
   "source": [
    "---\n",
    "\n",
    "## TESTING\n",
    "\n",
    "Model trained on generated data is tested on the real MNIST test dataset to evaluate utility.\n",
    "\n",
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ms0HrSyw6KLi"
   },
   "outputs": [],
   "source": [
    "(X_test_org, Y_test_org) = test_dataset, test_labels\n",
    "\n",
    "X_test_org = test_dataset\n",
    "X_test_org = (X_test_org - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "\n",
    "Y_test_org = [int(y) for y in Y_test_org]\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "Y_test_org = label_binarize(Y_test_org, classes=classes)\n",
    "\n",
    "X_test = layers.Flatten()(X_test_org)\n",
    "Y_test = layers.Flatten()(Y_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pDoU5x9eWln"
   },
   "source": [
    "### Get test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "OhHFs0M85Yng",
    "outputId": "f34911ed-a9bf-4797-da24-240074d6ce5c"
   },
   "outputs": [],
   "source": [
    "##### Vanilla Neural Network\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "classifier_NN = OneVsRestClassifier(MLPClassifier(random_state=2, alpha=1))\n",
    "NN_model2 = classifier_NN.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_NBzTjRcGws"
   },
   "outputs": [],
   "source": [
    "# ROC per class\n",
    "Y_score = NN_model2.predict_proba(X_test)\n",
    "false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(np.array(Y_test), Y_score)\n",
    "[str(au) + \" = \" + str(roc_auc[au]) for au in roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zZJJAJG07Zm2"
   },
   "outputs": [],
   "source": [
    "##### Logistic Regression\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "classifier_LR = OneVsRestClassifier(LogisticRegression(solver='lbfgs', \n",
    "                                                       multi_class='multinomial', \n",
    "                                                       random_state=2))\n",
    "LR_model2 = classifier_LR.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "uztxYfmLb_ge",
    "outputId": "e1c97e54-0a6f-439c-ab50-84a1eecee647"
   },
   "outputs": [],
   "source": [
    "# ROC per class\n",
    "Y_score = LR_model2.predict_proba(X_test)\n",
    "false_positive_rate, true_positive_rate, roc_auc = compute_fpr_tpr_roc(np.array(Y_test), Y_score)\n",
    "[str(au) + \" = \" + str(roc_auc[au]) for au in roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DP_CGAN_MNIST.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
